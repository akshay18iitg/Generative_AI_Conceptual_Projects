{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4ffa253-e427-4663-bc0b-9b1fcce508e6",
      "metadata": {
        "id": "d4ffa253-e427-4663-bc0b-9b1fcce508e6"
      },
      "source": [
        "# Rag From Scratch: Routing\n",
        "\n",
        "![image.png](attachment:c02ab9b5-38f9-451a-b202-62b54ab9c87a.png)\n",
        "\n",
        "## Enviornment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed",
      "metadata": {
        "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823ab5e8-a314-446e-9213-5f11cfd67555",
      "metadata": {
        "id": "823ab5e8-a314-446e-9213-5f11cfd67555"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc82800-5498-40be-86db-e1f6df8c86da",
      "metadata": {
        "id": "3bc82800-5498-40be-86db-e1f6df8c86da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acfd3be-d9d3-42f6-a13d-b936bb8cd6f4",
      "metadata": {
        "id": "9acfd3be-d9d3-42f6-a13d-b936bb8cd6f4"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e8ecbd-7ba1-4a2d-8ad5-4ce59e164d23",
      "metadata": {
        "id": "78e8ecbd-7ba1-4a2d-8ad5-4ce59e164d23"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fd9558-c53f-4a6c-80f0-c31b3bfd55de",
      "metadata": {
        "id": "45fd9558-c53f-4a6c-80f0-c31b3bfd55de"
      },
      "source": [
        "## Part 10: Logical and Semantic routing\n",
        "\n",
        "Use function-calling for classification.\n",
        "\n",
        "Flow:\n",
        "\n",
        "![Screenshot 2024-03-15 at 3.29.30 PM.png](attachment:b6699c4f-6188-4e0e-8ba4-21582dbca9ef.png)\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/query_analysis/techniques/routing#routing-to-multiple-indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c2cf60-d636-4992-a021-1236f7688999",
      "metadata": {
        "id": "04c2cf60-d636-4992-a021-1236f7688999",
        "outputId": "313c891a-8f58-4d8f-a1c4-08201f8914aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rlm/miniforge3/envs/llama2/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb5aaa3-7826-471c-9203-390009cbb4d6",
      "metadata": {
        "id": "1cb5aaa3-7826-471c-9203-390009cbb4d6"
      },
      "source": [
        "Note: we used function calling to produce structured output.\n",
        "\n",
        "![Screenshot 2024-03-16 at 12.38.23 PM.png](attachment:1c7e2e9e-e85f-490f-9591-883a4070bdb2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc6febc-93df-49b4-9920-c93589ba021e",
      "metadata": {
        "id": "cfc6febc-93df-49b4-9920-c93589ba021e"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277536df-0904-4d99-92bb-652621afbdec",
      "metadata": {
        "id": "277536df-0904-4d99-92bb-652621afbdec",
        "outputId": "a2191b22-a732-4995-eb4a-0f0737126c37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouteQuery(datasource='python_docs')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
      "metadata": {
        "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
        "outputId": "cf6b8899-6067-48ca-aa80-d2836cd83e51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'python_docs'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.datasource"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba569c12-ba1d-4486-a5be-bc8c31a8448c",
      "metadata": {
        "id": "ba569c12-ba1d-4486-a5be-bc8c31a8448c"
      },
      "source": [
        "Once we have this, it is trivial to define a branch that uses `result.datasource`\n",
        "\n",
        "https://python.langchain.com/docs/expression_language/how_to/routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f15722-35c6-4456-ad1b-06463233db25",
      "metadata": {
        "id": "01f15722-35c6-4456-ad1b-06463233db25"
      },
      "outputs": [],
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
      "metadata": {
        "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
        "outputId": "9152c969-0dcb-4a22-d709-9796b350fabe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'chain for python_docs'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0afc28c6-6bb6-4869-8c4e-e81dd28c69fd",
      "metadata": {
        "id": "0afc28c6-6bb6-4869-8c4e-e81dd28c69fd"
      },
      "source": [
        "Trace:\n",
        "\n",
        "https://smith.langchain.com/public/c2ca61b4-3810-45d0-a156-3d6a73e9ee2a/r"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949",
      "metadata": {
        "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949"
      },
      "source": [
        "### Semantic routing\n",
        "\n",
        "Flow:\n",
        "\n",
        "![Screenshot 2024-03-15 at 3.30.08 PM.png](attachment:77626ada-cabe-4ecb-a8eb-22992883c5dc.png)\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/expression_language/cookbook/embedding_router"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
      "metadata": {
        "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
        "outputId": "3ac4b500-b8e4-4bd1-c2ce-f0b9801e2950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PHYSICS\n",
            "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. It is formed when a massive star collapses in on itself. The boundary surrounding a black hole is called the event horizon. Beyond the event horizon, the gravitational pull is so intense that even time and space are distorted. Black holes are some of the most mysterious and fascinating objects in the universe.\n"
          ]
        }
      ],
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Two prompts\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "embeddings = OpenAIEmbeddings()\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "def prompt_router(input):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    # Chosen prompt\n",
        "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"What's a black hole\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}